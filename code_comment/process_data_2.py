import json
import torch
import torch.utils.data as Data
from collections import Counter
import re

# åˆ†è¯å‡½æ•°ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰
def tokenize_zh(sentence):
    return list(sentence)  # æŒ‰å­—åˆ‡åˆ†

def tokenize_en(sentence):
    return re.findall(r"\b\w+\b", sentence.lower())  # ä»¥å•è¯ä¸ºå•ä½åˆ‡åˆ†

# è¯»å– JSON æ•°æ®å¹¶æ¸…ç†è¾“å…¥
def load_json(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            try:
                item = json.loads(line)
                item["input"] = item["input"].replace("è¯·ä½ æŠŠä¸­æ–‡ç¿»è¯‘æˆä¸ºè‹±æ–‡\n", "")  # ç§»é™¤å‰ç¼€
                data.append(item)
            except json.JSONDecodeError as e:
                print(f"JSON è§£æé”™è¯¯: {e}")
    return data

# æ„å»ºè¯æ±‡è¡¨
def build_vocab(sentences, tokenizer, min_freq=1):
    counter = Counter()
    for sentence in sentences:
        counter.update(tokenizer(sentence))

    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}
    vocab.update({"<PAD>": 0, "<UNK>": 1, "<SOS>": 2, "<EOS>": 3})  # é¢„ç•™ç‰¹æ®Šæ ‡è®°
    return vocab

# å°†å¥å­è½¬æ¢ä¸ºæ•°å€¼åŒ–å¼ é‡
def sentence_to_tensor(sentence, vocab, tokenizer, max_len):
    tokens = [vocab.get(word, vocab["<UNK>"]) for word in tokenizer(sentence)]
    tokens = [vocab["<SOS>"]] + tokens[: max_len - 2] + [vocab["<EOS>"]]
    tokens += [vocab["<PAD>"]] * (max_len - len(tokens))
    return torch.tensor(tokens, dtype=torch.long)

def make_data(src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len):
    enc_inputs = [sentence_to_tensor(sent, src_vocab, tokenize_zh, max_len) for sent in src_sentences]
    dec_inputs = [sentence_to_tensor(sent, tgt_vocab, tokenize_en, max_len) for sent in tgt_sentences]

    # ğŸš€ `dec_outputs` å»æ‰ <SOS>ï¼Œå¹¶ä¿æŒé•¿åº¦ä¸€è‡´
    dec_outputs = [torch.cat([tensor[1:], torch.tensor([tgt_vocab["<PAD>"]], dtype=torch.long)]) for tensor in dec_inputs]

    return torch.stack(enc_inputs), torch.stack(dec_inputs), torch.stack(dec_outputs)

# è‡ªå®šä¹‰æ•°æ®é›†
class MyDataset(Data.Dataset):
    def __init__(self, enc_inputs, dec_inputs, dec_outputs):
        self.enc_inputs = enc_inputs
        self.dec_inputs = dec_inputs
        self.dec_outputs = dec_outputs

    def __len__(self):
        return self.enc_inputs.size(0)

    def __getitem__(self, idx):
        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]

# collate_fn å¤„ç†ä¸åŒé•¿åº¦çš„æ‰¹æ¬¡æ•°æ®
def collate_fn(batch):
    enc_inputs, dec_inputs, dec_outputs = zip(*batch)
    return torch.stack(enc_inputs), torch.stack(dec_inputs), torch.stack(dec_outputs)


# è¯»å–æ•°æ®
file_path = 'train.json'
data = load_json(file_path)

# å¤„ç†å¥å­
src_sentences = [item['input'].lstrip("è¯·ä½ æŠŠä¸­æ–‡ç¿»è¯‘æˆä¸ºè‹±æ–‡\n") for item in data]  # åˆ é™¤å‰ç¼€
tgt_sentences = [item['output'] for item in data]

# æ„å»ºè¯æ±‡è¡¨
src_vocab = build_vocab(src_sentences, tokenize_zh)
tgt_vocab = build_vocab(tgt_sentences, tokenize_en)

src_vocab_size = len(src_vocab)
tgt_vocab_size = len(tgt_vocab)

# è®¡ç®—æœ€å¤§é•¿åº¦
max_len = max(max(len(tokenize_zh(s)) for s in src_sentences), max(len(tokenize_en(t)) for t in tgt_sentences)) + 2
