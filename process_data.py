import json
import torch
import torch.utils.data as Data
from collections import Counter
import jieba
import re
import string
import os

# è¯»å– JSON æ–‡ä»¶
def load_json_file(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            try:
                item = json.loads(line)
                data.append(item)
            except json.JSONDecodeError as e:
                print(f"Error parsing line: {line}")
    return data

# ä¸­æ–‡åˆ†è¯ï¼ˆä½¿ç”¨ jiebaï¼‰
def tokenize_zh(text):
    text = re.sub(r'[^\w\s]', '', text)  # å»é™¤æ‰€æœ‰æ ‡ç‚¹
    return list(jieba.cut(text))  # ç”¨ jieba è¿›è¡Œåˆ†è¯

# è‹±æ–‡åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼æ‹†åˆ†ï¼‰
def tokenize_en(text):
    text = text.lower().translate(str.maketrans('', '', string.punctuation))  # è½¬å°å†™ & å»æ ‡ç‚¹
    return text.split()  # æŒ‰ç©ºæ ¼æ‹†åˆ†

# æ„å»ºè¯æ±‡è¡¨
def build_vocab(sentences, tokenizer, vocab_size=50000, min_freq=2):
    counter = Counter()
    for sentence in sentences:
        tokens = tokenizer(sentence)
        counter.update(tokens)
    
    # é™åˆ¶è¯æ±‡è¡¨å¤§å°ï¼Œä½é¢‘å•è¯æ˜ å°„åˆ° <UNK>
    most_common = counter.most_common(vocab_size - 4)  # é¢„ç•™ <pad>, <s>, </s>, <unk>
    vocab = {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3}
    
    for word, freq in most_common:
        if freq >= min_freq:
            vocab[word] = len(vocab)

    return vocab

# è½¬æ¢ä¸ºç´¢å¼•
def make_data(src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=50):
    enc_inputs, dec_inputs, dec_outputs = [], [], []
    
    for src, tgt in zip(src_sentences, tgt_sentences):
        src_tokens = tokenize_zh(src)
        tgt_tokens = tokenize_en(tgt)

        # ç¼–ç å™¨è¾“å…¥
        enc_input = [src_vocab.get(word, src_vocab['<unk>']) for word in src_tokens]
        enc_input = [src_vocab['<s>']] + enc_input[:max_len-1]  # æ·»åŠ å¼€å§‹ç¬¦å·ï¼Œé™åˆ¶æœ€å¤§é•¿åº¦
        enc_input += [src_vocab['<pad>']] * (max_len - len(enc_input))

        # è§£ç å™¨è¾“å…¥
        dec_input = [tgt_vocab['<s>']] + [tgt_vocab.get(word, tgt_vocab['<unk>']) for word in tgt_tokens][:max_len-1]
        dec_input += [tgt_vocab['<pad>']] * (max_len - len(dec_input))

        # è§£ç å™¨è¾“å‡º
        dec_output = [tgt_vocab.get(word, tgt_vocab['<unk>']) for word in tgt_tokens][:max_len-1] + [tgt_vocab['</s>']]
        dec_output += [tgt_vocab['<pad>']] * (max_len - len(dec_output))

        enc_inputs.append(torch.tensor(enc_input, dtype=torch.long))
        dec_inputs.append(torch.tensor(dec_input, dtype=torch.long))
        dec_outputs.append(torch.tensor(dec_output, dtype=torch.long))

    return torch.stack(enc_inputs), torch.stack(dec_inputs), torch.stack(dec_outputs)

# æ•°æ®é›†ç±»
class MyDataset(Data.Dataset):
    def __init__(self, enc_inputs, dec_inputs, dec_outputs):
        super(MyDataset, self).__init__()
        self.enc_inputs = enc_inputs
        self.dec_inputs = dec_inputs
        self.dec_outputs = dec_outputs

    def __len__(self):
        return len(self.enc_inputs)

    def __getitem__(self, idx):
        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]

# Collate å‡½æ•°ï¼Œä¿è¯å¡«å……ä¸€è‡´
def collate_fn(batch):
    enc_inputs, dec_inputs, dec_outputs = zip(*batch)
    enc_inputs = torch.nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True, padding_value=0)
    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=0)
    dec_outputs = torch.nn.utils.rnn.pad_sequence(dec_outputs, batch_first=True, padding_value=0)
    return enc_inputs, dec_inputs, dec_outputs

# è¯»å–æ•°æ®
file_path = 'train.json'
data = load_json_file(file_path)

# å¤„ç†å¥å­
src_sentences = [item['input'].lstrip("è¯·ä½ æŠŠä¸­æ–‡ç¿»è¯‘æˆä¸ºè‹±æ–‡\n") for item in data]  # åˆ é™¤å‰ç¼€
tgt_sentences = [item['output'] for item in data]

# æ„å»ºè¯æ±‡è¡¨
src_vocab = build_vocab(src_sentences, tokenizer=tokenize_zh)
tgt_vocab = build_vocab(tgt_sentences, tokenizer=tokenize_en)

src_vocab_size = len(src_vocab)
tgt_vocab_size = len(tgt_vocab)

print(f"âœ… æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(src_vocab)}")
print(f"âœ… ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(tgt_vocab)}")

# ç”Ÿæˆ idx2word æ˜ å°„
idx2word = {idx: word for word, idx in tgt_vocab.items()}

# è®¾ç½®æœ€å¤§é•¿åº¦
max_len = 50  # å›ºå®šæœ€å¤§é•¿åº¦ï¼Œé¿å…è¿‡é•¿

# ç”Ÿæˆæ•°æ®
enc_inputs, dec_inputs, dec_outputs = make_data(src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len)

# æ„å»ºæ•°æ®é›†å’Œ DataLoader
dataset = MyDataset(enc_inputs, dec_inputs, dec_outputs)
dataloader = Data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)

print("ğŸ“Š è®­ç»ƒæ•°æ®é›†æ„å»ºå®Œæˆï¼")
print(f"ğŸ“ å¥å­æœ€å¤§é•¿åº¦: {max_len}")
print(f"ğŸ“ ç¤ºä¾‹æ•°æ®:\n  ğŸ”¹ enc_inputs[0]: {enc_inputs[0]}\n  ğŸ”¹ dec_inputs[0]: {dec_inputs[0]}\n  ğŸ”¹ dec_outputs[0]: {dec_outputs[0]}")
